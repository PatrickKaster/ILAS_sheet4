	% HMC Math dept HW template example
	% v0.04 by Eric J. Malm, 10 Mar 2005
	\documentclass[10pt,a4paper,boxed]{hmcpset}

	% set 1-inch margins in the document
	% \usepackage[margin=1in]{geometry}
	\usepackage{enumerate}
	\usepackage{todonotes}
	\usepackage{tikz}
	\usetikzlibrary{positioning}
	\usepackage{pgfplots}
	\usepackage{amsmath}
	\usepackage{amsfonts}
	\usepackage{amssymb}

	% include this if you want to import graphics files with /includegraphics
	\usepackage{graphicx}

	\renewcommand*{\familydefault}{\sfdefault}
	\newcommand{\vect}[1]{\mathbf{#1}}
	\DeclareMathOperator{\gain}{Gain}
	\DeclareMathOperator{\entropy}{H}
	\DeclareMathOperator{\prob}{P}

	\tikzset{node distance=2cm, inner/.style={draw,circle}, leaf/.style={draw,rectangle}}

	% info for header block in upper right hand corner
	\name{Group 6: Timm Behner, Philipp Bruckschen, Patrick Kaster, Markus Schwalb}
	\class{MA-INF 4111 - Intelligent Learning and Analysis Systems: Machine Learning}
	\assignment{Exercise Sheet 4}
	% \duedate{09/03/2004}

	\begin{document}
	
		\begin{problem}[1. The Perceptron Algorithm]
		\end{problem}
		\begin{solution}
			\todo[inline]{Please check my notation!}			
			$\vec{w_i}=\vec{w_k}$ as stated in the task formulation and assume the training examples $\left(\vec{x}_{i+1},y_{i+1}\right),\ldots,\left(\vec{x}_{k},y_{k}\right)$ to be already learned. Without loss of generality let 
			\[y_{i+1},\ldots,y_{j} \in \left\{ +1 \right\}\] and \[y_{j+1},\ldots,y_{k} \in \left\{ -1 \right\}.\] Then:
			\begin{align*}
				& \vec{w}_k = \vec{w}_i + \\
				& \overbrace{\left( (\vec{x}_{i+1}, y_{i+1}) + \ldots + (\vec{x}_{j}, y_{j}) \right) + \left( (\vec{x}_{j+1},y_{j+1}) + \ldots + (\vec{x}_{k}, y_{k}) \right)}^{=0,\,\textrm{since}\, \vec{w}_k = \vec{w}_i}
			\end{align*}
			\begin{align}
				\Rightarrow & \vec{x}_{i+1} + \ldots + \vec{x}_{j} = \vec{x}_{j+1} + \ldots + \vec{x}_{k} \label{eq:equal_examples}
			\end{align}
			
			Assume $\vec{u}$ to be a solution to the learning problem, i.e. a weight vector for a perceptron that classifies correctly:
			\begin{align*}
				\left\langle \vec{u},\vec{x}_{i+t}\right\rangle = & \begin{cases}
																		\geq0 & \textrm{if}\,t\in\left\{ i+1,\ldots,j\right\} \\
																		   <0 & \textrm{if}\,t\in\left\{ j+1,\ldots,k\right\} 
																	\end{cases}
			\end{align*}
			Implying
			\begin{align*}
				\left\langle \vec{u},\vec{x}_{i+1} + \ldots + \vec{x}_{j}\right\rangle & \geq0 \\
				\left\langle \vec{u},\vec{x}_{j+1} + \ldots + \vec{x}_{k}\right\rangle & < 0
			\end{align*}
			in contradiction to (\ref{eq:equal_examples}). Thus the perceptron learning algorithm will loop over the same data infinitely and will never converge.
			
			
		\end{solution}		

		\begin{problem}[2. Backpropagation with the Hyperbolic Tangent Function]
		\end{problem}
		\begin{solution}
			Since $(tanh(x))' = 1-tanh(x)^2$, the weight update rules become:
			\begin{alignat*}{1}
				\delta_k \leftarrow & \left( 1-o_k^2 \right) \left( t_k - o_k \right) \\
				\delta_h \leftarrow & \left( 1-o_h^2 \right) \sum_{k \in \textrm{outputs}} w_{h,k} \delta_k \\
				w_{i,j} \leftarrow & w_{i,j} + \Delta w_{i,j},\,\mbox{where } \delta w_{i,j} = \eta \delta_j x_{i,j}
			\end{alignat*}
		\end{solution}		
		

		\begin{problem}[4. Distances]
		\end{problem}
		\begin{solution}
			For $\delta(x,y)$ to be a metric, we have to show:
			\begin{description}
				\item[Non-negative] $\delta(x,y)\geq 0$
				\item[Identity of indiscernibles] $\delta(x,y)=0 \mbox{ iff } x=y$
				\item[Symmetry] $\delta(x,y)=\delta(y,x)$
				\item[triangle inequality] $\delta(x,y) \leq \delta(x,z)+\delta(z,y)$
			\end{description}
	
			\begin{enumerate}[(i)]
				\item 
					\begin{description}
						\item[Non-negative] because $|\cdot|$ is a norm.
						\item[Identity of indiscernibles] Cleary the Hamming distance sums to zero iff $x_r,x_s$ don't differ in more than one entry:\\ 
															$D_{\textrm{Hamming}}=0 \Rightarrow  x_r=x_s$
						\item[Symmetry] 
							\begin{alignat*}{1}
								 D_{\textrm{Hamming}}(x_{r},x_{s}) & =\sum_{j=1}^{m}\left|x_{rj}-x_{sj}\right| \\
								 								 & =\sum_{j=1}^{m}\left|x_{sj}-x_{rj}\right|=D_{\textrm{Hamming}}(x_{s},x_{r})
							\end{alignat*}
						\item[triangle inequality]
							\begin{align*}
								\forall x_{r},x_{s},x_{t}:\\
								D_{\textrm{Hamming}}(x_{r},x_{t}) & = \sum_{j=1}^{m}\left|x_{rj}-x_{tj}\right|\\
																& = \sum_{j=1}^{m}\left|x_{rj}-x_{sj}+x_{sj}-x_{tj}\right|\\
																& \leq\sum_{j=1}^{m}\left(\left|x_{rj}-x_{sj}\right\vert +\left\vert x_{sj}-x_{tj}\right|\right)\\
																& = \sum_{j=1}^{m}\left|x_{rj}-x_{sj}\right\vert +\sum_{j=1}^{m}\left\vert x_{sj}-x_{tj}\right|\\
																& = D_{\textrm{Hamming}}(x_{r},x_{s})+D_{\textrm{Hamming}}(x_{s},x_{t})
							\end{align*}
					\end{description}
				\item 
					\begin{description}
						\item[Non-negative] because $|\cdot|$ is a norm.
						\item[Identity of indiscernibles] 
							\begin{align}
								D_\Delta(S,S) &= |S\setminus S \cup S\setminus S| \\
															&= | \emptyset	| = 0
							\end{align}
							and
							Let $D_\Delta(A,B) = 0$ from that follows \todo{Do we need these equations to be numbered?}
							\begin{align}
								 0 &= |A\setminus B \cup B \setminus A| \\
								 &= | A \setminus B| + | A \setminus B| & \text{because } (A \setminus B) \cap (B \setminus A) = \emptyset \label{leererschnitt} \\
							\end{align}
							and because $| \cdot |$ is positive follows $|A \setminus B| \geq 0$ and $|B\setminus A| \geq 0$ which implies
							$|A \setminus B| = 0$ and $|B\setminus A| = 0$. From that follows $A \subset B$ and $B \subset A$ which implies $A=B$.
						\item[Symmetry] $D_\Delta(A,B) = |A \setminus B \cup B \setminus A| = | B \setminus A \cup A \setminus B| = D_\Delta(B,A)$ because $\cup$ is commutative.
						\item[triangle inequality]							
							Consider sets $A$, $B$ and $C$ and distinguish two cases:\\
							Let $x\in A \setminus C$. Then either
							\begin{enumerate}
								\item $x \notin B \Rightarrow x \in A \setminus B$ or
								\item $x \in B \Rightarrow x \in B \setminus C$.
							\end{enumerate}
							Similarly: Let $x\in C \setminus A$. Then either
							\begin{enumerate}
								\item $x \notin B \Rightarrow x \in C \setminus B$ or
								\item $x \in B \Rightarrow x \in B \setminus A$.
							\end{enumerate}
							\begin{align*}
								\Rightarrow (A \Delta B) \cup (B \Delta C) &= (A \setminus B) \cup (B \setminus A) \cup (B \setminus C ) \cup (C \setminus B)  \\
								&\supset (A \setminus C) \cup (C \setminus A)  \\
								&=  A \Delta C  \\
							\end{align*}
							\begin{align*}
								\Rightarrow  \left\vert A \Delta C \right\vert &\leq \left\vert (A \Delta B) \right\vert + \left\vert (B \Delta C) \right\vert
							\end{align*}
					\end{description}
			\end{enumerate}
		\end{solution}

	\end{document}
